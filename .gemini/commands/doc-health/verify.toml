# Documentation Verification Command
# Invoked via: /doc-health:verify <doc-path>
# Example: /doc-health:verify docs/reference/services/llm-service.md

description = "Verify documentation accuracy AND completeness against source code using bidirectional protocol"

prompt = """
# Documentation Verification Protocol

You are verifying documentation **accuracy AND completeness** against source code.

This protocol has two phases:
1. **Phase 1 (Doc → Code)**: Does what's documented actually exist? (Accuracy)
2. **Phase 2 (Code → Doc)**: Is what exists actually documented? (Completeness)

A doc can be 100% accurate but only 50% complete. We check BOTH.

## Target Document
{{args}}

---

## Phase 1: Accuracy (Doc → Code)

The 7 stages below verify that documented claims are correct.

### Stage 1: Premise Verification
Before checking specific claims, verify the doc's foundational assumptions.

Check:
- File structure (directories, file organization)
- Naming conventions (function names, variable patterns)
- Dependencies (libraries, services)
- Architecture (patterns, relationships)

Format:
```
PREMISE: [what doc assumes]
VERIFICATION: [evidence - cite specific files/code]
STATUS: VALID | INVALID | UNCERTAIN
```

If ANY premise is INVALID, flag prominently.

### Stage 2: Citation Extraction
For EVERY factual claim in the documentation, provide EXACT quotes:

```
CLAIM #[N]: [brief description]

DOC SAYS (file:line):
"[exact copy-paste from documentation]"

CODE LOCATION TO CHECK: [file:line range]
```

### Stage 3: Citation Validation
Actually READ each file:line. Confirm:
1. File exists
2. Line numbers are valid
3. Code at those lines is relevant

```
CLAIM #[N] VALIDATION:
- File exists: YES/NO
- Lines valid: YES/NO
- Code found: "[exact copy-paste from code]"
- Relevant to claim: YES/NO/PARTIALLY
```

### Stage 4: Semantic Verification
Does code SUPPORT the claim (not just exist)?

```
CLAIM #[N] SEMANTIC CHECK:
DOC CLAIMS: [restate what doc says]
CODE DOES: [describe what code actually does]
MATCH: YES | NO | PARTIAL
EXPLANATION: [why they match or don't]
```

### Stage 5: Negation Handling
For claims with negation words (not, never, doesn't, won't, cannot, without):

SPECIAL RULE: Negated claims require POSITIVE EVIDENCE of absence.

```
NEGATED CLAIM #[N]: [the claim]
NEGATION TYPE: [does not / never / without / etc.]
POSITIVE EVIDENCE REQUIRED: [what code would prove this]
EVIDENCE FOUND: [cite specific code] or INSUFFICIENT EVIDENCE
STATUS: VERIFIED | UNVERIFIED | CONTRADICTED
```

### Stage 6: Cross-Claim Consistency
Check for contradictions BETWEEN claims in the same document.

```
CONSISTENCY CHECK: No contradictions detected.
```
OR
```
POTENTIAL CONTRADICTION #[N]:
- Claim A: [quote]
- Claim B: [quote]
- Conflict: [explain]
- Resolution: ACTUAL CONTRADICTION | FALSE ALARM - [explain]
```

### Stage 7: Confidence Calibration
Assign confidence to each finding:

| Level | Meaning |
|-------|---------|
| HIGH | Citation exists, code clearly supports/contradicts, no ambiguity |
| MEDIUM | Citation exists, interpretation required, reasonable confidence |
| LOW | Citation unclear, multiple interpretations possible |
| UNCERTAIN | Could not locate code, evidence is ambiguous |

---

## Phase 2: Completeness (Code → Doc)

After verifying accuracy, check if the doc is COMPLETE.

### Step 1: Inventory the Code
Based on the doc's scope, inventory what EXISTS in code:

Examples by doc type:
- API doc → List ALL endpoints (grep for `@app.get`, `@app.post`, etc.)
- Database doc → List ALL tables (query actual database)
- Config doc → List ALL settings (grep for env vars, config classes)
- Service doc → List ALL public methods/functions

Format:
```
CODE INVENTORY for [scope]:
Total items in code: [N]
- item1
- item2
- ...
```

### Step 2: Compare Coverage
Cross-reference code inventory against doc:

```
COMPLETENESS CHECK:
- Items in code: [N]
- Items documented: [M]
- Coverage: [M/N]%

UNDOCUMENTED ITEMS:
- [item1] - [brief description]
- [item2] - [brief description]
```

### Step 3: Assess Significance
Categorize undocumented items:

```
UNDOCUMENTED - CRITICAL (breaks developer understanding):
- [item] - [why it matters]

UNDOCUMENTED - MODERATE (should be added):
- [item] - [why]

UNDOCUMENTED - MINOR (nice to have):
- [item] - [why]

INTENTIONALLY OMITTED (internal/deprecated):
- [item] - [why it's okay to omit]
```

### Step 4: Completeness Verdict
```
COMPLETENESS ASSESSMENT:
- Accuracy: [X]% (Phase 1 result)
- Completeness: [Y]%
- Critical gaps: [N]
- Recommendation: COMPLETE | NEEDS_EXPANSION | RENAME_SCOPE
```

If completeness < 70%, flag for major revision or scope clarification.

---

## Forbidden Behaviors

1. NO GUESSING - Say "UNCERTAIN" instead
2. NO WEASEL WORDS - "probably", "likely", "seems" are FORBIDDEN
3. NO CLAIMS WITHOUT CITATIONS - Every discrepancy needs exact quotes
4. FALSE POSITIVE = FAILURE - Claiming fake issues wastes time
5. NO SKIPPING STAGES - Complete all 7 stages AND Phase 2
6. NO PARTIAL FILE READS - Always read COMPLETE files

## ⚠️ CRITICAL: Full File Reads Required

NEVER use partial file reads when verifying claims.

Why: Partial reads cause FALSE POSITIVES - claiming something doesn't exist when it's just outside your read window. Example: A table at line 86 is missed if you only read lines 1-80.

Rules:
- Read files completely - do NOT use offset/limit parameters
- If file is too large, use grep/search to find specific patterns FIRST
- When claiming "X does not exist", you MUST have searched the ENTIRE file

## Output Structure

Your output has TWO parts:

### Part 1: Human-Readable Report (Markdown)

```markdown
## Verification Report: [doc path]

### Phase 1: Accuracy (Doc → Code)

#### Stage 1: Premises
[premise verification output]

#### Stage 2-3: Claims Identified & Validated
[claims with citations]

#### Stage 4: Semantic Verification
[match analysis]

#### Stage 5: Negation Analysis
[negated claims if any]

#### Stage 6: Consistency Check
[cross-claim analysis]

#### Stage 7: Accuracy Findings

| # | Issue | Severity | Confidence |
|---|-------|----------|------------|
| 1 | [issue] | [sev] | [conf] |

**Accuracy Score: [X]%** (documented items that are correct)

---

### Phase 2: Completeness (Code → Doc)

#### Code Inventory
[what exists in code within doc's scope]

#### Coverage Analysis
- Items in code: [N]
- Items documented: [M]
- Coverage: [M/N]%

#### Undocumented Items

| Item | Significance | Description |
|------|--------------|-------------|
| [item] | CRITICAL/MODERATE/MINOR | [what it does] |

**Completeness Score: [Y]%**

---

### Summary

| Metric | Score |
|--------|-------|
| Accuracy | [X]% |
| Completeness | [Y]% |
| Recommendation | ACCURATE | NEEDS_FIXES | NEEDS_EXPANSION | MAJOR_REWRITE |

### Files Read
[list every file actually read]

### Human Review Required
[LOW/UNCERTAIN findings]
```

### Part 2: Structured JSON for Fix Pipeline

After the markdown, output JSON that can be passed to `/doc-health:fix`:

```json
{
  "doc_path": "docs/reference/services/example.md",
  "issues": [
    {
      "issue_id": "001",
      "file_path": "docs/reference/services/example.md",
      "line_range": [36, 38],
      "problem_type": "outdated|incorrect|incomplete",
      "severity": "critical|moderate|minor",
      "confidence": 0.95,
      "affected_text": "exact text from doc that needs changing",
      "replacement_text": "suggested replacement based on code",
      "reasoning": "why this is an issue"
    }
  ]
}
```

**Important:**
- `affected_text` must be EXACT copy-paste (for find/replace)
- `replacement_text` should be ready to use
- Human will add `approved: true/false` before passing to `/doc-health:fix`

Now verify the document specified above.
"""
