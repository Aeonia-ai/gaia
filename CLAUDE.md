# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## üß† Development Philosophy & Problem-Solving Approach

**üõë STOP GUESSING - START RESEARCHING**
- When deployment issues occur, resist the urge to try random fixes
- Search for documented issues: `"[platform] [error-type] [technology] troubleshooting"`
- Example: "Fly.io internal DNS connection refused microservices"

**üî¨ SYSTEMATIC DIAGNOSIS**
- Test your hypothesis before implementing fixes
- Verify individual components work before assuming system-wide issues
- Use platform-specific debugging tools (e.g., `fly ssh console`)

**üìö PLATFORM-AWARE DEVELOPMENT**
- Every cloud platform has quirks - learn them early
- Example: Fly.io internal DNS is unreliable, AWS has eventual consistency, etc.
- Design around known platform limitations, don't fight them

**üö® RECOGNIZE ANTI-PATTERNS**
- "One step forward, one step back" = you're guessing, not diagnosing
- Timeouts during deployment = often infrastructure issues, not code issues
- Services work individually but not together = networking/discovery problem

**üìñ DOCUMENT SOLUTIONS & LESSONS**
- Capture both the specific fix AND the general approach that found it
- Future you will thank present you for documenting the "why" not just the "what"

## ‚ö†Ô∏è CRITICAL: Command Version Requirements

**ALWAYS use these command versions:**
- `docker compose` (with space) - NOT `docker-compose` (with hyphen)
- `fly postgres` or `fly mpg` - NOT `fly pg`
- `python -m pip` - NOT direct `pip`
- Test script - NOT direct `curl` commands

See [Command Reference](docs/command-reference.md) for complete list.

## Development Commands

### Portable Database Management

**‚ö†Ô∏è IMPORTANT: Use portable database scripts for environment consistency**

```bash
# Initialize database for any environment
./scripts/init-database-portable.sh --env dev --user admin@gaia.dev
./scripts/init-database-portable.sh --env staging --user admin@gaia.com --provider fly
./scripts/init-database-portable.sh --env prod --user admin@gaia.com --provider aws

# Apply schema migrations
./scripts/migrate-database.sh --env dev --migration migrations/001_add_feature.sql
./scripts/migrate-database.sh --env prod --migration migrations/001_add_feature.sql --dry-run

# Monitor database health and performance
./scripts/monitor-database.sh --env dev --report health
./scripts/monitor-database.sh --env prod --report performance
```

See [Portable Database Architecture](docs/portable-database-architecture.md) for complete details.

### Docker Compose Operations

**‚ö†Ô∏è IMPORTANT: Use `docker compose` (space) not `docker-compose` (hyphen)**

```bash
# Start all services
docker compose up

# Start specific services
docker compose up gateway auth-service
docker compose up db nats  # Infrastructure only

# Rebuild services
docker compose build
docker compose build --no-cache  # Force rebuild

# View logs
docker compose logs -f gateway
docker compose logs -f auth-service
```

See [Command Reference](docs/command-reference.md) for correct command syntax.

### Testing
```bash
# Run full test suite
docker compose run test

# Run specific test files
docker compose run test pytest tests/test_auth.py
docker compose run test pytest tests/test_integration.py

# Run with specific markers
docker compose run test pytest -m integration
docker compose run test pytest -m compatibility
```

### Local Docker Development
```bash
# Start full microservices stack locally
docker compose up

# Test local deployment with user-associated authentication
./scripts/test.sh --local providers        # API key: FJUeDkZRy0uPp7cYtavMsIfwi7weF9-RT7BeOlusqnE

# Gateway available at: http://localhost:8666 (LLM Platform compatible)
# Database auto-initialized with user: dev@gaia.local
```

### Development Workflow
```bash
# Initial setup
./scripts/setup.sh

# Smart testing (PREFERRED - use test script, not curl)
./scripts/test.sh --local health           # Local development
./scripts/test.sh --staging health         # Staging deployment
./scripts/test.sh --prod health            # Production deployment

# Environment-aware testing
./scripts/test.sh --local all              # Full local test suite
./scripts/test.sh --staging all            # Staging tests (expects some failures)

# Manual service health checks (if needed)
curl http://auth-service:8000/health
curl http://asset-service:8000/health
curl http://chat-service:8000/health
```

## Architecture Overview

### Microservices Structure
- **Gateway Service** (port 8666): Main entry point maintaining LLM Platform API compatibility
- **Auth Service**: JWT validation via Supabase, API key authentication
- **Asset Service**: Universal Asset Server functionality
- **Chat Service**: LLM interactions with MCP-agent workflows
- **Web Service** (port 8080): FastHTML frontend for chat interface

### Key Design Patterns
- **Backward Compatibility**: All API endpoints match LLM Platform exactly
- **User-Associated Authentication**: JWT tokens (users) + user-associated API keys (stored in database)
- **NATS Messaging**: Service coordination via subjects like `gaia.service.health`, `gaia.auth.*`
- **Shared Utilities**: Common functionality in `app/shared/`
- **Portable Database**: Same schema across local, dev, staging, production

### Service Communication
- **HTTP**: Direct service-to-service calls via configured URLs
- **NATS**: Event-driven coordination for health, auth events, processing updates
- **Database**: Shared PostgreSQL with LLM Platform-compatible schema

## Smart Scripts & Deployment

### Overview
Gaia Platform includes intelligent scripts that handle environment-aware testing, deployment, and management based on lessons learned from production deployments.

### Smart Testing Script (`scripts/test.sh`)
Environment-aware API testing with intelligent failure handling:

```bash
# Environment options
./scripts/test.sh --local all              # Full local testing
./scripts/test.sh --staging all            # Staging (expects partial failures)
./scripts/test.sh --prod all               # Production (expects full functionality)
./scripts/test.sh --url URL all            # Custom environment

# Individual tests
./scripts/test.sh --staging health         # Health check with context
./scripts/test.sh --local chat "Hello"     # Chat functionality
./scripts/test.sh --prod stream "Test"     # Streaming chat

# Test categories
./scripts/test.sh --local providers-all    # All provider endpoints
./scripts/test.sh --staging personas-all   # Persona management (may fail in staging)
./scripts/test.sh --prod performance-all   # Performance monitoring
```

**Key Features:**
- üåç **Environment Detection**: Automatically sets expectations per environment
- ‚ö†Ô∏è **Smart Failure Handling**: Staging failures marked as expected vs actual errors
- üé® **Color-coded Results**: Green (success), Yellow (expected failure), Red (error)
- üîë **Environment-specific Auth**: Different API keys per environment

### Smart Deployment Script (`scripts/deploy.sh`)
Intelligent deployment with lessons learned from cloud deployments:

```bash
# Basic deployments
./scripts/deploy.sh --env staging                    # Gateway-only deployment
./scripts/deploy.sh --env production --services all  # Full microservices

# Advanced options
./scripts/deploy.sh --env staging --region lax --rebuild
./scripts/deploy.sh --env production --database fly --services "gateway auth"
```

**Deployment Patterns:**
- üöÄ **Gateway-Only**: Fast deployment, embedded services, NATS disabled
- üèóÔ∏è **Full Microservices**: Independent services, NATS enabled, service mesh
- üåé **Co-located Database**: Fly.io Postgres in same region for <1ms latency
- üîê **Secret Management**: Automatic .env secret deployment to Fly.io

### Management Script (`scripts/manage.sh`)
Comprehensive platform management combining deploy, test, and monitor:

```bash
# Deployment workflows
./scripts/manage.sh deploy-and-test staging    # Deploy + comprehensive test
./scripts/manage.sh status                     # Overview of all environments

# Testing workflows  
./scripts/manage.sh quick-test production      # Fast health checks
./scripts/manage.sh full-test staging          # Complete test suite

# Operations
./scripts/manage.sh monitor staging            # Real-time monitoring
./scripts/manage.sh scale production gateway 5 # Scale to 5 instances
./scripts/manage.sh logs staging gateway       # Stream logs
./scripts/manage.sh rollback staging           # Emergency rollback
```

**Smart Features:**
- üß™ **Deploy-and-Test**: Automated deployment with validation
- üìä **Multi-Environment Status**: Real-time health across local/staging/prod
- üîÑ **Zero-Downtime Operations**: Safe scaling and rollbacks
- üì± **Environment-Aware Testing**: Different test expectations per environment

### Cloud Deployment Lessons Learned

**Database Co-location:**
```toml
# Optimal: Both app and database in same region
app = 'gaia-gateway-staging'
primary_region = 'lax'

# Database URL: Co-located Fly.io Postgres in LAX
DATABASE_URL = "postgresql://postgres:...@direct.xxx.flympg.net:5432/postgres"
```

**NATS Configuration:**
```toml
# Local development: Full NATS coordination
NATS_URL = "nats://localhost:4222"

# Cloud deployment: NATS disabled for gateway-only pattern
NATS_URL = "disabled"
```

**Service Expectations by Environment:**
- **Local**: Full microservices, all endpoints working
- **Staging**: Gateway-only, asset/persona endpoints may fail (expected)
- **Production**: All services operational, full functionality

## Fly.io Deployment Configuration

### Organization Setup
The Gaia Platform is deployed under the **aeonia-dev** organization on Fly.io:
- **Organization Name**: aeonia-dev
- **Organization Type**: SHARED
- **Primary Region**: LAX (Los Angeles)

### Database Management
Fly.io has migrated to Managed Postgres (mpg). Key commands:

```bash
# List managed Postgres databases
fly mpg list

# Create new managed Postgres database
fly postgres create \
  --name gaia-db-production \
  --region lax \
  --vm-size shared-cpu-1x \
  --volume-size 10 \
  --initial-cluster-size 1 \
  --org aeonia-dev

# Connect to database
fly postgres connect -a gaia-db-production

# Get connection string
fly postgres connect -a gaia-db-production --command "echo \$DATABASE_URL"
```

### Existing Infrastructure
- **Staging Database**: Check with `fly mpg list` for existing databases
- **Production Database**: May already exist - verify before creating new one
- **Database Naming**: gaia-db-{environment} (e.g., gaia-db-staging, gaia-db-production)

## Configuration

### Environment Setup
1. Copy `.env.example` to `.env`
2. Configure required fields:
   - `API_KEY`: Primary authentication key
   - `SUPABASE_URL`, `SUPABASE_ANON_KEY`, `SUPABASE_JWT_SECRET`: Supabase auth
   - `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`: LLM providers
   - Asset generation APIs: `STABILITY_API_KEY`, `MESHY_API_KEY`, etc.

### Service URLs (Inter-service Communication)
- Gateway: `http://gateway:8000` (external: `localhost:8666`)
- Auth: `http://auth-service:8000`
- Asset: `http://asset-service:8000`
- Chat: `http://chat-service:8000`
- Web: `http://web-service:8000` (external: `localhost:8080`)
- Redis: `redis://redis:6379` (caching layer)

## Shared Modules

### Core Utilities (`app/shared/`)
- **`config.py`**: Centralized Pydantic settings with environment variable management
- **`security.py`**: Authentication patterns, JWT validation, `AuthenticationResult` class
- **`database.py`**: SQLAlchemy setup with connection pooling
- **`logging.py`**: Color-coded structured logging with custom levels (NETWORK, SERVICE, NATS)
- **`nats_client.py`**: Async messaging client with reconnection handling
- **`redis_client.py`**: Redis caching client with connection pooling and fallback handling
- **`supabase.py`**: Supabase client integration

### Authentication Patterns
- Use `get_current_auth_legacy()` for LLM Platform compatibility
- Inter-service calls use `validate_auth_for_service()`
- Auth service provides `/internal/validate` for service coordination

## Development Guidelines

### Adding New Services
1. Create service directory in `app/services/`
2. Implement FastAPI app with `/health` endpoint
3. Add Dockerfile and docker-compose service
4. Configure service URL in `.env.example` and `config.py`
5. Add NATS message patterns if needed

### Testing Requirements
- Unit tests: `pytest -m unit`
- Integration tests: `pytest -m integration` 
- Compatibility tests: `pytest -m compatibility` (verify LLM Platform API compatibility)
- Performance tests: Redis caching validation and timing comparisons
- Use test markers defined in `pytest.ini`

### Performance Optimization (In Progress)

**Current Task**: Implementing quick-win optimizations from `/docs/optimization-guide.md`

**Completed**:
- ‚úÖ Redis caching system (97% performance improvement locally)
- ‚úÖ Local database indexing (composite index for `personas(is_active, created_at DESC)`)

**In Progress** (Priority order):
1. **Response compression middleware** - Add GZipMiddleware to all services (30-50% response size reduction)
2. **Database connection pooling** - Increase pool sizes for better concurrency
3. **Cache TTL optimization** - Extend TTLs for stable data (reduce DB load by 50-70%)

**Next Steps**:
- Apply database indexes to production database when connectivity available
- Container multi-stage builds for smaller images
- API pagination for large responses

**Expected Impact**: 40-60% overall performance improvement with ~1.5 hours total effort

**Notes**: 
- All optimizations maintain backward compatibility
- Changes are incremental and low-risk
- Performance gains validated locally, ready for production

### Client Compatibility
- Maintain identical API endpoints to LLM Platform
- All existing clients (Unity XR, Unity Mobile AR, Unreal Engine, NextJS) must work unchanged
- Gateway service handles routing and maintains response formats

## NATS Messaging Subjects

### Service Coordination
- `gaia.service.health`: Service health status updates
- `gaia.service.ready`: Service startup coordination

### Domain Events
- `gaia.auth.*`: Authentication events (login, logout, validation)
- `gaia.asset.generation.*`: Asset processing lifecycle events
- `gaia.chat.*`: Chat processing events

### Event Models
- `ServiceHealthEvent`: Service status with metadata
- `AssetGenerationEvent`: Asset processing lifecycle
- `ChatMessageEvent`: Chat processing coordination

## Troubleshooting

### Common Issues
```bash
# Services won't start
docker info  # Check Docker status
docker compose build --no-cache

# Database connection issues
docker compose logs db
docker compose down -v && docker compose up db

# NATS connection problems
docker compose logs nats
docker compose exec nats nats-server --help
```

### Fly.io Internal DNS Issues

‚ö†Ô∏è **Common Problem**: Gateway returns "Service unavailable" or 503 errors for working services.

**Symptoms:**
- Gateway health check shows "degraded" status
- Individual services are healthy when tested directly
- Error: "Failed to connect to service-name.internal"

**Root Cause**: Fly.io's `.internal` DNS occasionally stops working (documented issue)

**Diagnosis:**
```bash
# 1. Test gateway health
./scripts/test.sh --url https://gaia-gateway-dev.fly.dev health
# Shows: "status": "degraded"

# 2. Test individual services directly
curl -H "X-API-Key: API_KEY" https://gaia-chat-dev.fly.dev/health
# Shows: working fine

# 3. Test internal DNS from gateway
fly ssh console -a gaia-gateway-dev --command "curl -I http://gaia-chat-dev.internal:8000/health"
# Shows: "Failed to connect to gaia-chat-dev.internal"
```

**Solution**: Switch from internal DNS to public URLs
```bash
# Fix gateway service URLs
fly secrets set -a gaia-gateway-dev \
  "CHAT_SERVICE_URL=https://gaia-chat-dev.fly.dev" \
  "AUTH_SERVICE_URL=https://gaia-auth-dev.fly.dev" \
  "ASSET_SERVICE_URL=https://gaia-asset-dev.fly.dev"

# Wait for deployment, then test
./scripts/test.sh --url https://gaia-gateway-dev.fly.dev health
# Should show: "status": "healthy"
```

**Prevention**: Always use public URLs for service-to-service communication on Fly.io
```bash
# ‚ùå Unreliable internal DNS
CHAT_SERVICE_URL=http://gaia-chat-dev.internal:8000

# ‚úÖ Reliable public URLs  
CHAT_SERVICE_URL=https://gaia-chat-dev.fly.dev
```

**References**: 
- [Fly.io Internal DNS Issues](https://community.fly.io/t/internal-dns-occasionally-stops-working-for-some-apps/5748)
- [Fly.io Private Networking Docs](https://fly.io/docs/networking/private-networking/)

### Debugging Methodology

When facing deployment issues, follow this systematic approach:

**üõë STOP GUESSING** - Research first, then fix

1. **Research the Problem**
   ```bash
   # Don't guess - look up the actual issue
   # Search: "platform-name problem-description troubleshooting"
   # Example: "Fly.io internal DNS connection refused microservices"
   ```

2. **Test Your Hypothesis**
   ```bash
   # Verify the diagnosis before implementing fixes
   fly ssh console -a app-name --command "test-command"
   curl -I http://service.internal:8000/health  # Test internal connectivity
   ```

3. **Apply Documented Solutions**
   ```bash
   # Use official workarounds, not custom hacks
   # Example: Use public URLs instead of internal DNS (documented Fly.io issue)
   ```

4. **Verify the Fix**
   ```bash
   # Test that the solution actually works
   ./scripts/test.sh --url URL health
   ./scripts/test.sh --url URL chat "test message"
   ```

**Key Lesson**: "One step forward, one step back" cycles usually mean you're guessing instead of researching the actual problem.

### Performance Monitoring
```bash
# Resource usage
docker stats

# Service health (use smart test script)
./scripts/test.sh --local health

# Smart deployment management
./scripts/manage.sh status              # Overview of all environments
./scripts/manage.sh quick-test staging  # Quick health checks
./scripts/manage.sh monitor staging     # Detailed monitoring

# NATS monitoring
curl localhost:8222/varz  # NATS HTTP monitoring
```

## FastHTML Web Service Integration

### Overview
The Web Service provides a FastHTML-based chat interface that integrates with all Gaia microservices while maintaining the visual design and functionality of the React client.

### Architecture
```
FastHTML Frontend (8080) ‚Üí Gateway (8666) ‚Üí Services
‚îú‚îÄ User login ‚Üí Auth Service ‚Üí Supabase validation
‚îú‚îÄ Chat messages ‚Üí Chat Service ‚Üí LLM Platform logic
‚îú‚îÄ Real-time updates ‚Üí NATS ‚Üí WebSocket to frontend
‚îî‚îÄ Asset requests ‚Üí Asset Service ‚Üí External APIs
```

### Key Features
- **Server-Side Rendering**: FastHTML components with Tailwind CSS
- **HTMX Integration**: Dynamic updates without complex JavaScript
- **Service Integration**: Uses existing Gaia microservices via Gateway
- **Real-time Chat**: WebSocket + NATS for live updates
- **Unified Auth**: Same Supabase JWT system as API clients

### Development Commands
```bash
# Start with web service
docker compose up web-service

# Access points
# FastHTML frontend: http://localhost:8080
# API gateway: http://localhost:8666

# Web service logs
docker compose logs -f web-service

# Test web service health
curl http://localhost:8080/health
```

### Directory Structure
```
app/services/web/
‚îú‚îÄ‚îÄ main.py              # FastHTML application
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ gaia_ui.py       # Complete UI component library (extracted from React)
‚îÇ   ‚îú‚îÄ‚îÄ auth.py          # Authentication components
‚îÇ   ‚îú‚îÄ‚îÄ chat.py          # Chat interface components
‚îÇ   ‚îî‚îÄ‚îÄ layout.py        # Layout and UI components
‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îú‚îÄ‚îÄ auth.py          # Authentication routes
‚îÇ   ‚îú‚îÄ‚îÄ chat.py          # Chat functionality routes
‚îÇ   ‚îî‚îÄ‚îÄ api.py           # Gateway API integration
‚îî‚îÄ‚îÄ static/
    ‚îî‚îÄ‚îÄ gaia-design.css  # Design system CSS (extracted from React)
```

### API Integration Pattern
```python
# Web service communicates with other services via Gateway
class GaiaAPIClient:
    def __init__(self):
        self.gateway_url = settings.GATEWAY_URL  # http://gateway:8000
    
    async def authenticate_user(self, email: str, password: str):
        # Routes through Gateway ‚Üí Auth Service ‚Üí Supabase
        return await self.client.post(f"{self.gateway_url}/api/v1/auth/validate")
    
    async def send_message(self, conversation_id: str, message: str, auth_token: str):
        # Routes through Gateway ‚Üí Chat Service ‚Üí LLM Platform
        return await self.client.post(f"{self.gateway_url}/api/v1/chat/completions")
```

### Real-time Integration
- **WebSocket Endpoint**: `/ws/{conversation_id}` for live chat updates
- **NATS Subscription**: Listens to `gaia.chat.*` events for real-time coordination
- **Event-Driven**: New messages broadcast via NATS to all connected clients

### Session Management
- **FastHTML Sessions**: Built-in session middleware for user authentication
- **JWT Integration**: Stores Supabase JWT tokens in secure sessions
- **Auth Decorator**: `@require_auth` for protected routes

## Implementation Status & Future Tasks

### ‚úÖ Completed Systems

#### Provider Management System (20/22 endpoints) - FULLY COMPLETE
- **Core functionality**: Multi-LLM provider support (Claude, OpenAI)
- **Health monitoring**: Real-time provider health and response times
- **Model management**: List, filter, and get detailed model information
- **Usage statistics**: Track requests, tokens, costs, error rates
- **Authentication**: Full API key validation through gateway
- **Removed by design**: Model recommendation and comparison endpoints

#### Streaming Chat System (10 endpoints) - FULLY COMPLETE
- **Server-Sent Events**: OpenAI/Anthropic compatible streaming
- **Multi-provider support**: Intelligent model selection
- **Performance optimized**: Sub-700ms response times for VR/AR
- **Caching system**: 0ms access time after first request

#### Core Infrastructure - FULLY COMPLETE
- **Gateway service**: Request routing and authentication
- **Database**: PostgreSQL with SQLAlchemy 2.0
- **NATS messaging**: Service coordination
- **Authentication**: JWT + API key support

### üîß Next Priority Systems

#### 1. Asset Pricing System (18 endpoints) - HIGH PRIORITY
- Revenue and cost management functionality
- Asset generation pricing models
- Usage tracking and billing

#### 2. Persona Management CRUD (7 endpoints) - MEDIUM PRIORITY  
- User experience personalization
- Custom persona creation and management
- Persona memory and context

#### 3. Performance Monitoring (6 endpoints) - MEDIUM PRIORITY
- System health metrics
- Response time monitoring
- Error rate tracking

#### 4. Auth Service Enhancement (2 endpoints) - LOW PRIORITY
- User registration endpoints
- Login/logout functionality

### üö® Technical Debt to Address Later

#### Provider System Enhancements
- **Full LLM Registry**: Replace simple endpoints with complete provider registry system
- **Dynamic provider registration**: Add/remove providers at runtime
- **Advanced model selection**: Context-aware intelligent model selection
- **Cost optimization**: Real-time cost tracking and budget limits

#### Infrastructure Improvements
- **Service discovery**: Automatic service registration via NATS
- **Circuit breakers**: Fault tolerance for external API calls
- **Rate limiting**: Per-user and per-provider rate limits
- **Monitoring**: Comprehensive metrics and alerting

#### Security & Compliance
- **API key rotation**: Automatic key rotation and management
- **Audit logging**: Comprehensive request/response logging
- **Data retention**: Configurable data retention policies
- **Compliance**: GDPR/CCPA compliance features

### üéØ Current Focus
Moving to **Asset Pricing System** implementation to enable revenue management and cost tracking across the platform.

### Deployment Integration
- **Docker Service**: Runs as `web-service` in docker-compose.yml
- **Port Mapping**: External port 8080, internal port 8000
- **Service Dependencies**: Depends on gateway, auth-service, chat-service
- **Shared Volumes**: Access to same data volumes as other services

### Design System Integration

The web service includes a complete design system extracted from the React client:

**Visual Elements**:
- **Color Scheme**: Purple/pink gradients (`from-purple-600 to-pink-600`), slate backgrounds
- **Butterfly Logo**: ü¶ã emoji with gradient background (`from-purple-400 via-pink-400 to-blue-400`)
- **Typography**: White text on dark backgrounds, purple accent colors
- **Layout**: 320px sidebar, centered chat area, responsive design

**Component Library** (`app/services/web/components/gaia_ui.py`):
```python
# Pre-built components matching React versions exactly
gaia_logo(size="small")           # Butterfly logo with gradient
gaia_button("Text", variant="primary")  # Purple/pink gradient buttons
gaia_auth_form(is_login=True)     # Complete auth form
gaia_message_bubble(content, role="user")  # Chat message styling
gaia_sidebar_header()             # Sidebar with logo and new chat button
```

**Design Tokens**:
```python
class GaiaDesign:
    BG_MAIN = "bg-gradient-to-br from-indigo-950 via-purple-900 to-slate-900"
    BTN_PRIMARY = "bg-gradient-to-r from-purple-600 to-pink-600"
    LOGO_EMOJI = "ü¶ã"
    LOGO_BG = "bg-gradient-to-br from-purple-400 via-pink-400 to-blue-400"
```

**CSS Classes** (`app/services/web/static/gaia-design.css`):
- `.gaia-bg`: Main gradient background
- `.gaia-logo-small`, `.gaia-logo-large`: Logo styling
- `.gaia-btn-primary`: Button gradients
- `.gaia-message-user`, `.gaia-message-assistant`: Message bubble styling

### Benefits
1. **Zero API Disruption**: Existing API clients continue working unchanged
2. **Service Reuse**: Leverages all existing Gaia microservices
3. **Consistent Auth**: Same Supabase authentication across web and API
4. **Real-time**: NATS enables live chat updates
5. **Independent Scaling**: Web service scales separately from API services
6. **Modern SSR**: Server-side rendered with progressive enhancement
7. **Visual Parity**: Pixel-perfect match to React client design
8. **Design System**: Extracted and reusable UI components

# Microservices Scaling Architecture

## üöÄ Scaling Advantages Over Monolithic LLM Platform

### Independent Service Scaling

The Gaia Platform's microservices architecture provides significant scaling advantages over the original monolithic LLM Platform:

#### Before (Monolith):
```
üè¢ Single Giant Server
‚îú‚îÄ Chat + Auth + Assets + Monitoring ALL together
‚îú‚îÄ Scale everything or scale nothing
‚îú‚îÄ One traffic spike = entire system overwhelmed
‚îî‚îÄ $$$$ Expensive over-provisioning
```

#### After (Gaia Microservices):
```
üèóÔ∏è Independent Scaling per Service
‚îú‚îÄ Chat Service: 5 instances (high chat traffic)
‚îú‚îÄ Auth Service: 2 instances (stable auth load)  
‚îú‚îÄ Asset Service: 10 instances (heavy image generation)
‚îú‚îÄ Performance Service: 1 instance (monitoring only)
‚îî‚îÄ Gateway: 3 instances (load balancing)
```

### Workload-Specific Scaling

```yaml
# Example: Black Friday Asset Generation Spike
asset-service:
  replicas: 20        # Scale up for image generation
  memory: "4Gi"       # GPU-heavy workloads
  
chat-service:
  replicas: 3         # Normal chat traffic
  memory: "1Gi"       # Keep costs low

auth-service:
  replicas: 2         # Authentication is stable
  memory: "512Mi"     # Minimal resources needed
```

### Technology-Specific Optimization
- **Chat Service**: GPU instances for LLM inference
- **Asset Service**: High-memory instances for image generation
- **Auth Service**: Small, fast instances for JWT validation
- **Performance Service**: Monitoring-optimized instances

## Real-World Scaling Scenarios

### Scenario 1: VR Game Launch Spike üéÆ
```
Problem: 10,000 simultaneous VR users hit streaming chat
Monolith: Entire system crashes, auth/assets/monitoring all die
Gaia: Only scale chat-service to 50 instances, others unaffected
Cost: $50/hour vs $500/hour for full monolith scaling
```

### Scenario 2: Asset Generation Explosion üé®
```
Problem: Viral TikTok trend creates 100x image generation requests
Monolith: Chat users can't login because asset processing overwhelms system
Gaia: Scale asset-service independently, chat/auth remain responsive
Result: Revenue keeps flowing while handling asset spike
```

### Scenario 3: Global Expansion üåç
```
Problem: Expanding to Asia-Pacific region
Monolith: Deploy entire heavy system in each region
Gaia: 
‚îú‚îÄ Deploy lightweight auth/gateway globally
‚îú‚îÄ Keep heavy chat-service in fewer regions
‚îú‚îÄ Replicate asset-service only where needed
‚îî‚îÄ Share performance monitoring globally
```

## Performance Scaling Multipliers

### Database Scaling
```python
# Monolith: One database for everything
single_db = "All services fight for same connection pool"

# Gaia: Service-specific optimization
auth_db = "Fast SSD, optimized for auth queries"
chat_db = "High-memory, optimized for conversation history"  
asset_db = "Large storage, optimized for metadata"
performance_db = "Time-series optimized for metrics"
```

### Service-Specific Caching
```python
chat_service:
  redis_cache: "LLM response caching, 10GB memory"
  
auth_service:
  redis_cache: "JWT validation cache, 1GB memory"
  
asset_service:
  s3_cache: "Generated asset cache, 1TB storage"
```

## Cost Efficiency Gains

### Development Team Scaling
```
Monolith Team Structure:
‚îú‚îÄ 10 developers all working on same codebase
‚îú‚îÄ Constant merge conflicts and coordination overhead
‚îú‚îÄ Deploy entire system for any change
‚îî‚îÄ Testing requires full system spin-up

Gaia Team Structure:
‚îú‚îÄ Chat Team (3 devs): Focus on LLM optimization
‚îú‚îÄ Auth Team (2 devs): Focus on security and performance  
‚îú‚îÄ Asset Team (3 devs): Focus on generation pipelines
‚îú‚îÄ Platform Team (2 devs): Focus on infrastructure
‚îî‚îÄ Independent deploys, testing, and releases
```

### Infrastructure Cost Optimization
```yaml
# Development Environment
Monolith: 1 √ó $200/month large instance = $200/month
Gaia: 4 √ó $30/month small instances = $120/month

# Production Environment  
Monolith: 5 √ó $500/month instances = $2,500/month
Gaia: Smart scaling based on actual usage = $1,200/month average
  Peak: chat√ó10, asset√ó5, auth√ó2, perf√ó1 = $2,000/month
  Normal: chat√ó3, asset√ó2, auth√ó2, perf√ó1 = $800/month
```

## Reliability & Fault Isolation

### Failure Cascade Prevention
```
Monolith Failure:
Asset generation bug ‚Üí Memory leak ‚Üí Entire system crashes
Result: Users can't login, chat, or access anything

Gaia Failure Isolation:
Asset generation bug ‚Üí Only asset-service crashes  
Result: Users can still chat, login, monitor performance
Auto-restart: Asset service recovers in 30 seconds
```

### Zero-Downtime Deployments
```bash
# Monolith: All or nothing deployment
deploy_monolith: "Entire system down for 5 minutes"

# Gaia: Zero-downtime rolling deployments
kubectl rolling-update chat-service    # Chat users unaffected
kubectl rolling-update asset-service   # Asset generation briefly slower  
kubectl rolling-update auth-service    # New logins briefly delayed
# Never full system downtime!
```

## Performance Metrics Comparison

### Response Time Optimization
```python
# Monolith bottlenecks
chat_response_time = "2.5s"  # Affected by asset processing load
auth_response_time = "800ms" # Affected by chat traffic
asset_response_time = "15s"  # Affected by everything

# Gaia optimization  
chat_response_time = "400ms" # Dedicated resources
auth_response_time = "100ms" # Lightweight, dedicated
asset_response_time = "12s"  # Can scale independently
```

### Throughput Multiplication
```
Monolith Maximum:
‚îú‚îÄ 100 concurrent chat requests
‚îú‚îÄ 20 concurrent asset generations  
‚îú‚îÄ 500 concurrent auth requests
‚îî‚îÄ Shared resources limit everything

Gaia Maximum:
‚îú‚îÄ 1000+ concurrent chat requests (dedicated scaling)
‚îú‚îÄ 200+ concurrent asset generations (GPU instances)
‚îú‚îÄ 5000+ concurrent auth requests (lightweight instances) 
‚îî‚îÄ Each service scales to its hardware limits
```

## Future Scaling Possibilities

### Service Specialization
```python
# Easy to add specialized services
persona_ai_service = "Dedicated GPU inference for persona generation"
voice_synthesis_service = "Dedicated audio processing instances"
image_analysis_service = "Computer vision specialized hardware"
real_time_collaboration = "WebSocket-optimized instances"
```

### Geographic Distribution
```yaml
# Edge deployment possibilities
us_west:
  chat_service: "Low latency for US users"
  
asia_pacific:  
  asset_service: "Local image generation"
  
europe:
  auth_service: "GDPR compliance region"
  
global:
  performance_service: "Worldwide monitoring"
```

## Kubernetes Scaling Configuration

### Horizontal Pod Autoscaling
```yaml
# Chat service auto-scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: chat-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: chat-service
  minReplicas: 2
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### Vertical Pod Autoscaling
```yaml
# Asset service resource optimization
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: asset-service-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: asset-service
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: asset-service
      maxAllowed:
        cpu: 8
        memory: 16Gi
      minAllowed:
        cpu: 100m
        memory: 512Mi
```

## The Scaling Bottom Line

**Monolith**: Scale everything expensively or suffer performance  
**Gaia**: Scale smartly, fail gracefully, optimize continuously

The microservices architecture transforms scaling from a **"big expensive problem"** into **"targeted, efficient solutions"** - enabling 10x traffic handling with 50% cost reduction through intelligent resource allocation and fault isolation.