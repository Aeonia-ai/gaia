# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Development Commands

### Docker Compose Operations
```bash
# Start all services
docker-compose up

# Start specific services
docker-compose up gateway auth-service
docker-compose up db nats  # Infrastructure only

# Rebuild services
docker-compose build
docker-compose build --no-cache  # Force rebuild

# View logs
docker-compose logs -f gateway
docker-compose logs -f auth-service
```

### Testing
```bash
# Run full test suite
docker-compose run test

# Run specific test files
docker-compose run test pytest tests/test_auth.py
docker-compose run test pytest tests/test_integration.py

# Run with specific markers
docker-compose run test pytest -m integration
docker-compose run test pytest -m compatibility
```

### Development Workflow
```bash
# Initial setup
./scripts/setup.sh

# Health check
curl http://localhost:8666/health

# Manual service health checks
curl http://auth-service:8000/health
curl http://asset-service:8000/health
curl http://chat-service:8000/health
```

## Architecture Overview

### Microservices Structure
- **Gateway Service** (port 8666): Main entry point maintaining LLM Platform API compatibility
- **Auth Service**: JWT validation via Supabase, API key authentication
- **Asset Service**: Universal Asset Server functionality (planned)
- **Chat Service**: LLM interactions with MCP-agent workflows (planned)
- **Web Service** (port 8080): FastHTML frontend for chat interface (planned)

### Key Design Patterns
- **Backward Compatibility**: All API endpoints match LLM Platform exactly
- **Dual Authentication**: JWT tokens (users) + API keys (services)
- **NATS Messaging**: Service coordination via subjects like `gaia.service.health`, `gaia.auth.*`
- **Shared Utilities**: Common functionality in `app/shared/`

### Service Communication
- **HTTP**: Direct service-to-service calls via configured URLs
- **NATS**: Event-driven coordination for health, auth events, processing updates
- **Database**: Shared PostgreSQL with LLM Platform-compatible schema

## Configuration

### Environment Setup
1. Copy `.env.example` to `.env`
2. Configure required fields:
   - `API_KEY`: Primary authentication key
   - `SUPABASE_URL`, `SUPABASE_ANON_KEY`, `SUPABASE_JWT_SECRET`: Supabase auth
   - `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`: LLM providers
   - Asset generation APIs: `STABILITY_API_KEY`, `MESHY_API_KEY`, etc.

### Service URLs (Inter-service Communication)
- Gateway: `http://gateway:8000` (external: `localhost:8666`)
- Auth: `http://auth-service:8000`
- Asset: `http://asset-service:8000`
- Chat: `http://chat-service:8000`
- Web: `http://web-service:8000` (external: `localhost:8080`)

## Shared Modules

### Core Utilities (`app/shared/`)
- **`config.py`**: Centralized Pydantic settings with environment variable management
- **`security.py`**: Authentication patterns, JWT validation, `AuthenticationResult` class
- **`database.py`**: SQLAlchemy setup with connection pooling
- **`logging.py`**: Color-coded structured logging with custom levels (NETWORK, SERVICE, NATS)
- **`nats_client.py`**: Async messaging client with reconnection handling
- **`supabase.py`**: Supabase client integration

### Authentication Patterns
- Use `get_current_auth_legacy()` for LLM Platform compatibility
- Inter-service calls use `validate_auth_for_service()`
- Auth service provides `/internal/validate` for service coordination

## Development Guidelines

### Adding New Services
1. Create service directory in `app/services/`
2. Implement FastAPI app with `/health` endpoint
3. Add Dockerfile and docker-compose service
4. Configure service URL in `.env.example` and `config.py`
5. Add NATS message patterns if needed

### Testing Requirements
- Unit tests: `pytest -m unit`
- Integration tests: `pytest -m integration` 
- Compatibility tests: `pytest -m compatibility` (verify LLM Platform API compatibility)
- Use test markers defined in `pytest.ini`

### Client Compatibility
- Maintain identical API endpoints to LLM Platform
- All existing clients (Unity XR, Unity Mobile AR, Unreal Engine, NextJS) must work unchanged
- Gateway service handles routing and maintains response formats

## NATS Messaging Subjects

### Service Coordination
- `gaia.service.health`: Service health status updates
- `gaia.service.ready`: Service startup coordination

### Domain Events
- `gaia.auth.*`: Authentication events (login, logout, validation)
- `gaia.asset.generation.*`: Asset processing lifecycle events
- `gaia.chat.*`: Chat processing events

### Event Models
- `ServiceHealthEvent`: Service status with metadata
- `AssetGenerationEvent`: Asset processing lifecycle
- `ChatMessageEvent`: Chat processing coordination

## Troubleshooting

### Common Issues
```bash
# Services won't start
docker info  # Check Docker status
docker-compose build --no-cache

# Database connection issues
docker-compose logs db
docker-compose down -v && docker-compose up db

# NATS connection problems
docker-compose logs nats
docker-compose exec nats nats-server --help
```

### Performance Monitoring
```bash
# Resource usage
docker stats

# Service health
curl localhost:8666/health

# NATS monitoring
curl localhost:8222/varz  # NATS HTTP monitoring
```

## FastHTML Web Service Integration

### Overview
The Web Service provides a FastHTML-based chat interface that integrates with all Gaia microservices while maintaining the visual design and functionality of the React client.

### Architecture
```
FastHTML Frontend (8080) ‚Üí Gateway (8666) ‚Üí Services
‚îú‚îÄ User login ‚Üí Auth Service ‚Üí Supabase validation
‚îú‚îÄ Chat messages ‚Üí Chat Service ‚Üí LLM Platform logic
‚îú‚îÄ Real-time updates ‚Üí NATS ‚Üí WebSocket to frontend
‚îî‚îÄ Asset requests ‚Üí Asset Service ‚Üí External APIs
```

### Key Features
- **Server-Side Rendering**: FastHTML components with Tailwind CSS
- **HTMX Integration**: Dynamic updates without complex JavaScript
- **Service Integration**: Uses existing Gaia microservices via Gateway
- **Real-time Chat**: WebSocket + NATS for live updates
- **Unified Auth**: Same Supabase JWT system as API clients

### Development Commands
```bash
# Start with web service
docker-compose up web-service

# Access points
# FastHTML frontend: http://localhost:8080
# API gateway: http://localhost:8666

# Web service logs
docker-compose logs -f web-service

# Test web service health
curl http://localhost:8080/health
```

### Directory Structure
```
app/services/web/
‚îú‚îÄ‚îÄ main.py              # FastHTML application
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ gaia_ui.py       # Complete UI component library (extracted from React)
‚îÇ   ‚îú‚îÄ‚îÄ auth.py          # Authentication components
‚îÇ   ‚îú‚îÄ‚îÄ chat.py          # Chat interface components
‚îÇ   ‚îî‚îÄ‚îÄ layout.py        # Layout and UI components
‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îú‚îÄ‚îÄ auth.py          # Authentication routes
‚îÇ   ‚îú‚îÄ‚îÄ chat.py          # Chat functionality routes
‚îÇ   ‚îî‚îÄ‚îÄ api.py           # Gateway API integration
‚îî‚îÄ‚îÄ static/
    ‚îî‚îÄ‚îÄ gaia-design.css  # Design system CSS (extracted from React)
```

### API Integration Pattern
```python
# Web service communicates with other services via Gateway
class GaiaAPIClient:
    def __init__(self):
        self.gateway_url = settings.GATEWAY_URL  # http://gateway:8000
    
    async def authenticate_user(self, email: str, password: str):
        # Routes through Gateway ‚Üí Auth Service ‚Üí Supabase
        return await self.client.post(f"{self.gateway_url}/api/v1/auth/validate")
    
    async def send_message(self, conversation_id: str, message: str, auth_token: str):
        # Routes through Gateway ‚Üí Chat Service ‚Üí LLM Platform
        return await self.client.post(f"{self.gateway_url}/api/v1/chat/completions")
```

### Real-time Integration
- **WebSocket Endpoint**: `/ws/{conversation_id}` for live chat updates
- **NATS Subscription**: Listens to `gaia.chat.*` events for real-time coordination
- **Event-Driven**: New messages broadcast via NATS to all connected clients

### Session Management
- **FastHTML Sessions**: Built-in session middleware for user authentication
- **JWT Integration**: Stores Supabase JWT tokens in secure sessions
- **Auth Decorator**: `@require_auth` for protected routes

## Implementation Status & Future Tasks

### ‚úÖ Completed Systems

#### Provider Management System (20/22 endpoints) - FULLY COMPLETE
- **Core functionality**: Multi-LLM provider support (Claude, OpenAI)
- **Health monitoring**: Real-time provider health and response times
- **Model management**: List, filter, and get detailed model information
- **Usage statistics**: Track requests, tokens, costs, error rates
- **Authentication**: Full API key validation through gateway
- **Removed by design**: Model recommendation and comparison endpoints

#### Streaming Chat System (10 endpoints) - FULLY COMPLETE
- **Server-Sent Events**: OpenAI/Anthropic compatible streaming
- **Multi-provider support**: Intelligent model selection
- **Performance optimized**: Sub-700ms response times for VR/AR
- **Caching system**: 0ms access time after first request

#### Core Infrastructure - FULLY COMPLETE
- **Gateway service**: Request routing and authentication
- **Database**: PostgreSQL with SQLAlchemy 2.0
- **NATS messaging**: Service coordination
- **Authentication**: JWT + API key support

### üîß Next Priority Systems

#### 1. Asset Pricing System (18 endpoints) - HIGH PRIORITY
- Revenue and cost management functionality
- Asset generation pricing models
- Usage tracking and billing

#### 2. Persona Management CRUD (7 endpoints) - MEDIUM PRIORITY  
- User experience personalization
- Custom persona creation and management
- Persona memory and context

#### 3. Performance Monitoring (6 endpoints) - MEDIUM PRIORITY
- System health metrics
- Response time monitoring
- Error rate tracking

#### 4. Auth Service Enhancement (2 endpoints) - LOW PRIORITY
- User registration endpoints
- Login/logout functionality

### üö® Technical Debt to Address Later

#### Provider System Enhancements
- **Full LLM Registry**: Replace simple endpoints with complete provider registry system
- **Dynamic provider registration**: Add/remove providers at runtime
- **Advanced model selection**: Context-aware intelligent model selection
- **Cost optimization**: Real-time cost tracking and budget limits

#### Infrastructure Improvements
- **Service discovery**: Automatic service registration via NATS
- **Circuit breakers**: Fault tolerance for external API calls
- **Rate limiting**: Per-user and per-provider rate limits
- **Monitoring**: Comprehensive metrics and alerting

#### Security & Compliance
- **API key rotation**: Automatic key rotation and management
- **Audit logging**: Comprehensive request/response logging
- **Data retention**: Configurable data retention policies
- **Compliance**: GDPR/CCPA compliance features

### üéØ Current Focus
Moving to **Asset Pricing System** implementation to enable revenue management and cost tracking across the platform.

### Deployment Integration
- **Docker Service**: Runs as `web-service` in docker-compose.yml
- **Port Mapping**: External port 8080, internal port 8000
- **Service Dependencies**: Depends on gateway, auth-service, chat-service
- **Shared Volumes**: Access to same data volumes as other services

### Design System Integration

The web service includes a complete design system extracted from the React client:

**Visual Elements**:
- **Color Scheme**: Purple/pink gradients (`from-purple-600 to-pink-600`), slate backgrounds
- **Butterfly Logo**: ü¶ã emoji with gradient background (`from-purple-400 via-pink-400 to-blue-400`)
- **Typography**: White text on dark backgrounds, purple accent colors
- **Layout**: 320px sidebar, centered chat area, responsive design

**Component Library** (`app/services/web/components/gaia_ui.py`):
```python
# Pre-built components matching React versions exactly
gaia_logo(size="small")           # Butterfly logo with gradient
gaia_button("Text", variant="primary")  # Purple/pink gradient buttons
gaia_auth_form(is_login=True)     # Complete auth form
gaia_message_bubble(content, role="user")  # Chat message styling
gaia_sidebar_header()             # Sidebar with logo and new chat button
```

**Design Tokens**:
```python
class GaiaDesign:
    BG_MAIN = "bg-gradient-to-br from-indigo-950 via-purple-900 to-slate-900"
    BTN_PRIMARY = "bg-gradient-to-r from-purple-600 to-pink-600"
    LOGO_EMOJI = "ü¶ã"
    LOGO_BG = "bg-gradient-to-br from-purple-400 via-pink-400 to-blue-400"
```

**CSS Classes** (`app/services/web/static/gaia-design.css`):
- `.gaia-bg`: Main gradient background
- `.gaia-logo-small`, `.gaia-logo-large`: Logo styling
- `.gaia-btn-primary`: Button gradients
- `.gaia-message-user`, `.gaia-message-assistant`: Message bubble styling

### Benefits
1. **Zero API Disruption**: Existing API clients continue working unchanged
2. **Service Reuse**: Leverages all existing Gaia microservices
3. **Consistent Auth**: Same Supabase authentication across web and API
4. **Real-time**: NATS enables live chat updates
5. **Independent Scaling**: Web service scales separately from API services
6. **Modern SSR**: Server-side rendered with progressive enhancement
7. **Visual Parity**: Pixel-perfect match to React client design
8. **Design System**: Extracted and reusable UI components

# Microservices Scaling Architecture

## üöÄ Scaling Advantages Over Monolithic LLM Platform

### Independent Service Scaling

The Gaia Platform's microservices architecture provides significant scaling advantages over the original monolithic LLM Platform:

#### Before (Monolith):
```
üè¢ Single Giant Server
‚îú‚îÄ Chat + Auth + Assets + Monitoring ALL together
‚îú‚îÄ Scale everything or scale nothing
‚îú‚îÄ One traffic spike = entire system overwhelmed
‚îî‚îÄ $$$$ Expensive over-provisioning
```

#### After (Gaia Microservices):
```
üèóÔ∏è Independent Scaling per Service
‚îú‚îÄ Chat Service: 5 instances (high chat traffic)
‚îú‚îÄ Auth Service: 2 instances (stable auth load)  
‚îú‚îÄ Asset Service: 10 instances (heavy image generation)
‚îú‚îÄ Performance Service: 1 instance (monitoring only)
‚îî‚îÄ Gateway: 3 instances (load balancing)
```

### Workload-Specific Scaling

```yaml
# Example: Black Friday Asset Generation Spike
asset-service:
  replicas: 20        # Scale up for image generation
  memory: "4Gi"       # GPU-heavy workloads
  
chat-service:
  replicas: 3         # Normal chat traffic
  memory: "1Gi"       # Keep costs low

auth-service:
  replicas: 2         # Authentication is stable
  memory: "512Mi"     # Minimal resources needed
```

### Technology-Specific Optimization
- **Chat Service**: GPU instances for LLM inference
- **Asset Service**: High-memory instances for image generation
- **Auth Service**: Small, fast instances for JWT validation
- **Performance Service**: Monitoring-optimized instances

## Real-World Scaling Scenarios

### Scenario 1: VR Game Launch Spike üéÆ
```
Problem: 10,000 simultaneous VR users hit streaming chat
Monolith: Entire system crashes, auth/assets/monitoring all die
Gaia: Only scale chat-service to 50 instances, others unaffected
Cost: $50/hour vs $500/hour for full monolith scaling
```

### Scenario 2: Asset Generation Explosion üé®
```
Problem: Viral TikTok trend creates 100x image generation requests
Monolith: Chat users can't login because asset processing overwhelms system
Gaia: Scale asset-service independently, chat/auth remain responsive
Result: Revenue keeps flowing while handling asset spike
```

### Scenario 3: Global Expansion üåç
```
Problem: Expanding to Asia-Pacific region
Monolith: Deploy entire heavy system in each region
Gaia: 
‚îú‚îÄ Deploy lightweight auth/gateway globally
‚îú‚îÄ Keep heavy chat-service in fewer regions
‚îú‚îÄ Replicate asset-service only where needed
‚îî‚îÄ Share performance monitoring globally
```

## Performance Scaling Multipliers

### Database Scaling
```python
# Monolith: One database for everything
single_db = "All services fight for same connection pool"

# Gaia: Service-specific optimization
auth_db = "Fast SSD, optimized for auth queries"
chat_db = "High-memory, optimized for conversation history"  
asset_db = "Large storage, optimized for metadata"
performance_db = "Time-series optimized for metrics"
```

### Service-Specific Caching
```python
chat_service:
  redis_cache: "LLM response caching, 10GB memory"
  
auth_service:
  redis_cache: "JWT validation cache, 1GB memory"
  
asset_service:
  s3_cache: "Generated asset cache, 1TB storage"
```

## Cost Efficiency Gains

### Development Team Scaling
```
Monolith Team Structure:
‚îú‚îÄ 10 developers all working on same codebase
‚îú‚îÄ Constant merge conflicts and coordination overhead
‚îú‚îÄ Deploy entire system for any change
‚îî‚îÄ Testing requires full system spin-up

Gaia Team Structure:
‚îú‚îÄ Chat Team (3 devs): Focus on LLM optimization
‚îú‚îÄ Auth Team (2 devs): Focus on security and performance  
‚îú‚îÄ Asset Team (3 devs): Focus on generation pipelines
‚îú‚îÄ Platform Team (2 devs): Focus on infrastructure
‚îî‚îÄ Independent deploys, testing, and releases
```

### Infrastructure Cost Optimization
```yaml
# Development Environment
Monolith: 1 √ó $200/month large instance = $200/month
Gaia: 4 √ó $30/month small instances = $120/month

# Production Environment  
Monolith: 5 √ó $500/month instances = $2,500/month
Gaia: Smart scaling based on actual usage = $1,200/month average
  Peak: chat√ó10, asset√ó5, auth√ó2, perf√ó1 = $2,000/month
  Normal: chat√ó3, asset√ó2, auth√ó2, perf√ó1 = $800/month
```

## Reliability & Fault Isolation

### Failure Cascade Prevention
```
Monolith Failure:
Asset generation bug ‚Üí Memory leak ‚Üí Entire system crashes
Result: Users can't login, chat, or access anything

Gaia Failure Isolation:
Asset generation bug ‚Üí Only asset-service crashes  
Result: Users can still chat, login, monitor performance
Auto-restart: Asset service recovers in 30 seconds
```

### Zero-Downtime Deployments
```bash
# Monolith: All or nothing deployment
deploy_monolith: "Entire system down for 5 minutes"

# Gaia: Zero-downtime rolling deployments
kubectl rolling-update chat-service    # Chat users unaffected
kubectl rolling-update asset-service   # Asset generation briefly slower  
kubectl rolling-update auth-service    # New logins briefly delayed
# Never full system downtime!
```

## Performance Metrics Comparison

### Response Time Optimization
```python
# Monolith bottlenecks
chat_response_time = "2.5s"  # Affected by asset processing load
auth_response_time = "800ms" # Affected by chat traffic
asset_response_time = "15s"  # Affected by everything

# Gaia optimization  
chat_response_time = "400ms" # Dedicated resources
auth_response_time = "100ms" # Lightweight, dedicated
asset_response_time = "12s"  # Can scale independently
```

### Throughput Multiplication
```
Monolith Maximum:
‚îú‚îÄ 100 concurrent chat requests
‚îú‚îÄ 20 concurrent asset generations  
‚îú‚îÄ 500 concurrent auth requests
‚îî‚îÄ Shared resources limit everything

Gaia Maximum:
‚îú‚îÄ 1000+ concurrent chat requests (dedicated scaling)
‚îú‚îÄ 200+ concurrent asset generations (GPU instances)
‚îú‚îÄ 5000+ concurrent auth requests (lightweight instances) 
‚îî‚îÄ Each service scales to its hardware limits
```

## Future Scaling Possibilities

### Service Specialization
```python
# Easy to add specialized services
persona_ai_service = "Dedicated GPU inference for persona generation"
voice_synthesis_service = "Dedicated audio processing instances"
image_analysis_service = "Computer vision specialized hardware"
real_time_collaboration = "WebSocket-optimized instances"
```

### Geographic Distribution
```yaml
# Edge deployment possibilities
us_west:
  chat_service: "Low latency for US users"
  
asia_pacific:  
  asset_service: "Local image generation"
  
europe:
  auth_service: "GDPR compliance region"
  
global:
  performance_service: "Worldwide monitoring"
```

## Kubernetes Scaling Configuration

### Horizontal Pod Autoscaling
```yaml
# Chat service auto-scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: chat-service-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: chat-service
  minReplicas: 2
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### Vertical Pod Autoscaling
```yaml
# Asset service resource optimization
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: asset-service-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: asset-service
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: asset-service
      maxAllowed:
        cpu: 8
        memory: 16Gi
      minAllowed:
        cpu: 100m
        memory: 512Mi
```

## The Scaling Bottom Line

**Monolith**: Scale everything expensively or suffer performance  
**Gaia**: Scale smartly, fail gracefully, optimize continuously

The microservices architecture transforms scaling from a **"big expensive problem"** into **"targeted, efficient solutions"** - enabling 10x traffic handling with 50% cost reduction through intelligent resource allocation and fault isolation.